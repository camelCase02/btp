{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow==2.5.0 scikit-learn optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dummy_npwarn_decorator_factory():\n",
        "  def npwarn_decorator(x):\n",
        "    return x\n",
        "  return npwarn_decorator\n",
        "np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tf.__version__)\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mlp_model(input_shape,hidden_layer_one,dropout_one,hidden_layer_two,dropout_two):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_layer_one, activation='elu', input_shape=input_shape))\n",
        "    if dropout_one !=0:\n",
        "        model.add(Dropout(dropout_one))\n",
        "    model.add(Dense(hidden_layer_two, activation='elu'))\n",
        "    if dropout_two !=0:\n",
        "        model.add(Dropout(dropout_two))\n",
        "    model.add(Dense(1))\n",
        "    # Compile the model\n",
        "    model.compile(loss='mean_squared_error', optimizer=\"Adam\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_error(trues, predicted):\n",
        "    corr = np.corrcoef(predicted, trues)[0,1]\n",
        "    mae = np.mean(np.abs(predicted - trues))\n",
        "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
        "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
        "    rrse = np.sqrt(np.sum((predicted - trues)**2) / np.sum((trues - np.mean(trues))**2))\n",
        "    mape = np.mean(np.abs((predicted - trues) / trues)) * 100\n",
        "    r2 = max(0, 1 - np.sum((predicted - trues)**2) / np.sum((trues - np.mean(trues))**2))\n",
        "    # Calculez les autres mesures d'erreur ici\n",
        "    return rmse, corr, mae, rae, rrse, mape, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C0BkZXt65Ez",
        "outputId": "23a1bc40-2ac9-4a90-9040-db4f19ee38d6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import optuna\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3bTXF_XcinP",
        "outputId": "f76c02a0-521f-4017-c706-a3cf084c3307"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F0LEKbphcinQ"
      },
      "source": [
        "# We start by loading the pre-processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iok0znZ9cinS",
        "outputId": "13141a83-357f-41a0-df1b-e57456394b51"
      },
      "outputs": [],
      "source": [
        "order_history_data=pd.read_csv('../data/final_data.csv')\n",
        "order_history_data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "accsf3FX-psn"
      },
      "source": [
        "# Deep learning model optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "scn8bL42tsdw"
      },
      "outputs": [],
      "source": [
        "X = order_history_data.copy()\n",
        "y = order_history_data['Quantity']\n",
        "\n",
        "X_encoded = pd.get_dummies(X, columns=['Description', 'PartNo'])\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y, test_size=0.3, random_state=27)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=27)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9xS2bFBycinX"
      },
      "source": [
        "We're going to use mlp. To start with, we'll create an mlp with minimal parameters to get an idea of performance without model optimization. Then we'll optmize our model using Bayesian optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWk8p6_LcinX",
        "outputId": "692b591b-a8c3-4db4-c394-bcb350fd4cef"
      },
      "outputs": [],
      "source": [
        "\n",
        "best_mlp = MLPRegressor(hidden_layer_sizes=(50), activation='relu', alpha=0.008724528119026307, learning_rate='constant')\n",
        "\n",
        "# Train the model on the training data\n",
        "best_mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set with the trained model\n",
        "y_val_pred = best_mlp.predict(X_val)\n",
        "\n",
        "# Evaluate the performance of the model on the validation set\n",
        "mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "r2_val = best_mlp.score(X_val, y_val)\n",
        "\n",
        "# Make predictions on the test set with the trained model\n",
        "y_test_pred = best_mlp.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model on the test set\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "r2_test = best_mlp.score(X_test, y_test)\n",
        "\n",
        "print(\"Validation Set:\")\n",
        "print(\"MSE (Mean Squared Error) on the validation set:\", mse_val)\n",
        "print(\"R-squared (Coefficient of Determination) on the validation set:\", r2_val)\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(\"MSE (Mean Squared Error) on the test set:\", mse_test)\n",
        "print(\"R-squared (Coefficient of Determination) on the test set:\", r2_test)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "spupqHY9cinY"
      },
      "source": [
        "Using Bayesian optimization to find the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr-pESuVcinY",
        "outputId": "599630c9-c293-468e-d074-15241ec7bb5e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Define the hyperparameters to be optimized\n",
        "    hidden_layer_one = trial.suggest_categorical('hidden_layer_one', [32, 64, 128, 256,512])\n",
        "    hidden_layer_two = trial.suggest_categorical('hidden_layer_two', [32, 64, 128, 256,512])\n",
        "    dropout_one = trial.suggest_uniform('dropout_one', 0, 0.5)\n",
        "    dropout_two = trial.suggest_uniform('dropout_two', 0, 0.5)\n",
        "    # learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128,256,512])\n",
        "    epochs = trial.suggest_categorical('epochs', [200])\n",
        "\n",
        "\n",
        "\n",
        "    model=get_mlp_model(input_shape=(X_train.shape[1],),hidden_layer_one=hidden_layer_one,dropout_one=dropout_one,hidden_layer_two=hidden_layer_two,dropout_two=dropout_two)\n",
        "\n",
        "\n",
        "    # Define the ModelCheckpoint callback\n",
        "    checkpoint_filepath = 'model_checkpoint.h5'\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train the model on the training data with validation data and checkpoint callback\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0\n",
        "                        , validation_data=(X_val, y_val),\n",
        "                        callbacks=[model_checkpoint])\n",
        "\n",
        "    # Load the best weights from the saved checkpoint\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "\n",
        "    # Evaluate the performance of the model on the test set\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "    # rmse2, corr2, mae2, rae2, rrse2, mape2, r2_2 = compute_error(y_test.values, y_test_pred.reshape(y_test_pred.shape[0]))\n",
        "\n",
        "    return mse_test\n",
        "\n",
        "# Configure Optuna to use the GPU for exhaustive searches\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100, n_jobs=1)  # Use n_jobs=1 to avoid parallelism problems on GPU\n",
        "\n",
        "# Show best hyperparameters found\n",
        "print(\"Best hyperparameters:\")\n",
        "print(study.best_params)\n",
        "print(\"Best MSE:\", study.best_value)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MFAmNGI7cinY"
      },
      "source": [
        "Using Grid Search"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0fXRG0VmcinZ"
      },
      "source": [
        "Training model with best hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1l_cOCQyyXH",
        "outputId": "cc03c112-6f37-424a-8b44-1782197d1ff0"
      },
      "outputs": [],
      "source": [
        "# best parameters\n",
        "\n",
        "hidden_layer_one=50\n",
        "hidden_layer_two=100\n",
        "dropout_one=0\n",
        "dropout_two=0\n",
        "\n",
        "\n",
        "model=get_mlp_model(input_shape=(X_train.shape[1],),hidden_layer_one=hidden_layer_one,dropout_one=dropout_one,hidden_layer_two=hidden_layer_two,dropout_two=dropout_two)\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint_filepath = 'model_checkpoint.h5'\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "filepath=checkpoint_filepath,\n",
        "save_best_only=True,\n",
        "monitor='val_loss',\n",
        "mode='min',\n",
        "verbose=0\n",
        ")\n",
        "\n",
        "# Train the model on the training data with validation data and checkpoint callback\n",
        "history = model.fit(X_train, y_train, epochs=300, batch_size=512,\n",
        "            verbose=2, validation_data=(X_val, y_val),\n",
        "            callbacks=[model_checkpoint])\n",
        "\n",
        "# Load the best weights from the saved checkpoint\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Evaluate the performance of the model on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "rmse2, corr2, mae2, rae2, rrse2, mape2, r2_2 = compute_error(y_test.values, y_test_pred.reshape(y_test_pred.shape[0]))\n",
        "\n",
        "print(\"RMSE:\", rmse2)\n",
        "print(\"Corr√©lation:\", corr2)\n",
        "print(\"MAE:\", mae2)\n",
        "print(\"RAE:\", rae2)\n",
        "print(\"RRSE:\", rrse2)\n",
        "print(\"MAPE:\", mape2)\n",
        "print(\"R2:\", r2_2)\n",
        "print(\"----------------------------------\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AN-8AO2Ecina"
      },
      "source": [
        "Save the weights of the best model for later loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUAXH6qwi2By"
      },
      "outputs": [],
      "source": [
        "# Save the best model in checkpoint only if it is better\n",
        "model.save_weights('../checkpoint/base_model_checkpoint.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "curie",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
